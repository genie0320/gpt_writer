{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNA2ita/CROgIBZFA73ySm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/gpt_writer/blob/colab/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API_KEY 설정.\n",
        "- colab 환경 : 오른쪽 메뉴 > 열쇠모양(유저시크릿) > OPENAI_API_KEY로 설정 후 해당 코드실행."
      ],
      "metadata": {
        "id": "b16hMmRB8et1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "print(\"Got the key\" if openai_api_key else \"Something goes wrong\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dut4MoJcVTuf",
        "outputId": "611e739c-a95d-4c47-ab7b-26155848c607"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got the key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 50\n",
        "TEMP = 1.5\n",
        "# _______________"
      ],
      "metadata": {
        "id": "2y1TMKNK_9yi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lanchain 연결"
      ],
      "metadata": {
        "id": "X5YNSwFI9kCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain-openai"
      ],
      "metadata": {
        "id": "j9uCVp5M1ENA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4eb5c5-b3a4-4ddd-c2ee-da10cc055c3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export OPENAI_API_KEY=openai_api_key\n",
        "# set OPENAI_API_KEY=openai_api_key # on windows\n",
        "# %env OPENAI_API_KEY=openai_api_key # in colab"
      ],
      "metadata": {
        "id": "NZPig9muJrXE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(api_key=openai_api_key)\n",
        "chat_model = ChatOpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "BtIUojKEIWho"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understand Lanchain prompt Model.\n",
        "The LLM send / returns a **string**, while the ChatModel send / returns a **message**"
      ],
      "metadata": {
        "id": "y2j3i2L3Bhou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WfZdXgljcG9",
        "outputId": "8ddcdd7b-9dc1-4623-f9b8-b35cfc95c688"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schema"
      ],
      "metadata": {
        "id": "t56CD03JzBva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "# llm.invoke(\"hi?\")\n",
        "\n",
        "# LLM\n",
        "text = \"Hi?\"\n",
        "llm.invoke(text)\n",
        "\n",
        "# chatModel\n",
        "messages = [HumanMessage(content=text)]\n",
        "chat_model.invoke(messages)\n"
      ],
      "metadata": {
        "id": "SW4czU513NMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e8362b-1746-4973-bffa-a53cc9412778"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM prompt"
      ],
      "metadata": {
        "id": "O6vh42oszKOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"hi! {product}\")\n",
        "prompt.format(product=\"Nice to see you.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BjYvj1_qmYO2",
        "outputId": "10b5f047-995a-4ea0-a4b1-062a09d2eac2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi! Nice to see you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래와 같이 여러 조건을 덧붙일 수 있다."
      ],
      "metadata": {
        "id": "JmzZMmmD0jvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.prompts import PromptTemplate\n",
        "prompt = (\n",
        "    PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "    + \", make it funny\"\n",
        "    + \"\\n\\nand in {language}\"\n",
        ")"
      ],
      "metadata": {
        "id": "lRcdm_G_yevV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx7OZdT41dXq",
        "outputId": "ac6cba29-d2cb-41c2-fe5c-6cdcc99d4be0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(topic=\"sports\", language=\"korean\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NmCHX1cryesV",
        "outputId": "330457f2-c08e-40f2-c532-cf4efe0c5f90"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a joke about sports, make it funny\\n\\nand in korean'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat prompt"
      ],
      "metadata": {
        "id": "JYLgDlaYzQYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "template = \"You are my new male friend. We met {input_place} for {output_activity}.\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", human_template),\n",
        "])\n",
        "\n",
        "chat_prompt.format_messages(input_place=\"in library\", output_activity=\"being a study friend\", text=\"Hey, Sweety!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5vfwJfynM5P",
        "outputId": "2bd7762e-8d8b-42e6-fa2e-d981d72ee9e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are my new male friend. We met in library for being a study friend.'),\n",
              " HumanMessage(content='Hey, Sweety!')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output 의 형태를 정해줄 수도 있다. `\\n\\n{format_instructions}`  \n",
        "그런데 이건 output parser 에서도 정해줄 수 있다. 왜 여기서 굳이?"
      ],
      "metadata": {
        "id": "2VE_kK2Z0q7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\" # \\n\\n 는 왜 필요한거지?\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_template(template)\n",
        "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions()) # format_instructions에 따라서 출력해라 ? 굳이 왜 이렇게 하는지 모르겠음.\n",
        "chain = chat_prompt | chat_model | output_parser\n",
        "chain.invoke({\"text\": \"human_name\"}) # 위의 teamplate > text 에 human_name을 넣어서 대답을 가져와라."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "LOmbXdfapVe5",
        "outputId": "f6c23432-6ecd-416d-b157-aa1a2c878185"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output_parser' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f4a19f3764fe>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchat_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mchat_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_instructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_format_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# format_instructions에 따라서 출력해라 ? 굳이 왜 이렇게 하는지 모르겠음.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat_prompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mchat_model\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0moutput_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"human_name\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 위의 teamplate > text 에 human_name을 넣어서 대답을 가져와라.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'output_parser' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "# from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "9M9G8X_S1wF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(api_key=openai_api_key)\n",
        "chain = LLMChain(llm=model, prompt=prompt)\n",
        "chain.run(topic=\"sports\", language=\"korean\")"
      ],
      "metadata": {
        "id": "KwYe1h9C1wCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "VSWMU6QEWo2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding output parser"
      ],
      "metadata": {
        "id": "5OUS2oRrz5lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "output_parser.parse(\"hi, bye\")\n",
        "# >> ['hi', 'bye']"
      ],
      "metadata": {
        "id": "vLFfsyfkpFcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are new friend of mine\"),\n",
        "    (\"user\", \"{user_input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "RiO4m1S0332o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "SGquU3VA5ahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "vjzeRAn75ayX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_answered = chain.invoke({\"user_input\": \"Hi, there!\"})\n",
        "print(f'New friend from LLMs : {ai_answered}')"
      ],
      "metadata": {
        "id": "bxrEooe75qhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot prompt templates"
      ],
      "metadata": {
        "id": "NQUDxg7SEpRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Page link found | Content | As is link | to be link|\n",
        "------------------|-----------|---------|-----------|\n",
        "https://python.langchain.com/docs/modules/model_io/prompts | How to use few-shot examples with LLMs | https://python.langchain.com/docs/modules/model_io/few_shot_examples |https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples\n",
        ""
      ],
      "metadata": {
        "id": "O2dBQD-XwdvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a short {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"love\")"
      ],
      "metadata": {
        "id": "jUwARsnZHRbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "Kt5hmYn1E02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Hi, honey?\",\n",
        "        \"answer\": \"\"\"\n",
        "Yes,\n",
        "my love.\n",
        "When did you get here?\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"We just arrived. What should we eat?\",\n",
        "        \"answer\": \"\"\"\n",
        "Now I'm very hungry, so I think I can eat anything.\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Honey, What is your favorite animation these days?\",\n",
        "        \"answer\": \"\"\"\n",
        "You know I like \"Love Live\", right? A new season is on air these days.\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Really? Then do you want to go see it together?\",\n",
        "        \"answer\": \"\"\"\n",
        "No, it's okay. You're busy, so let's wait until the next movie comes out and watch it together.\n",
        "\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
        ")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))"
      ],
      "metadata": {
        "id": "GMOk9LqzE3oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For wordwrap for out cell."
      ],
      "metadata": {
        "id": "uqiEJjBe69uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # from ipywidgets import Textarea, widgets\n",
        "# # from IPython.display import display\n",
        "\n",
        "# output_to_display = res.content\n",
        "\n",
        "# # print(~) > res = ~     >> res.contents 등 가공. output_to_display >>\n",
        "# # 다시 위쪽의 코드에 반영.\n",
        "\n",
        "# output_widget = Textarea(\n",
        "#     value=output_to_display,\n",
        "#     layout=widgets.Layout(width=\"100%\", height='300px')  # Can change Height\n",
        "# )\n",
        "# display(output_widget)"
      ],
      "metadata": {
        "id": "tDnG-zxvaHLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function KeepClicking(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(KeepClicking,60000)"
      ],
      "metadata": {
        "id": "3YUr0T1Ae8Cw"
      }
    }
  ]
}