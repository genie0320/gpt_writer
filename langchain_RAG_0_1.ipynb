{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/gpt_writer/blob/colab/langchain_RAG_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project setting"
      ],
      "metadata": {
        "id": "b16hMmRB8et1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain langchain-openai\n",
        "!pip install --quiet pypdf chromadb tiktoken\n",
        "!pip install --quiet icecream"
      ],
      "metadata": {
        "id": "tqk3P_6g44hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import icecream as ic"
      ],
      "metadata": {
        "id": "WSKJtCL4jetK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Keys\n",
        "\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "# api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "holEwunaLCBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Drives\n",
        "\n",
        "# GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Fny7Ihe2MCn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444100c1-45cd-4e93-d2a1-3bc23258f7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 50\n",
        "TEMP = 1.5\n",
        "\n",
        "EMBED_MODEL = \"\"\n",
        "SPLIT = 500\n",
        "OVERRAP = 50\n",
        "# _______________\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = OpenAI(api_key=openai_api_key)\n",
        "chat_model = ChatOpenAI(api_key=openai_api_key)\n",
        "embed_model = ''"
      ],
      "metadata": {
        "id": "dut4MoJcVTuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"* Got the key\" if openai_api_key else \"Something goes wrong\")\n",
        "print(f'* LLM model set : \\n - model : {LLM_MODEL} \\n - max token : {MAX} \\n - temperature : {TEMP}')\n",
        "print(f'* Embed model set : \\n - model : {EMBED_MODEL} \\n - max token : {MAX} \\n - temperature : {TEMP}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaRGbFpQ7zzv",
        "outputId": "ab68168c-ced0-4684-deb6-0c15f1d6e03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Got the key\n",
            "* LLM model set : \n",
            " - model : gpt-3.5-turbo \n",
            " - max token : 50 \n",
            " - temperature : 1.5\n",
            "* Embed model set : \n",
            " - model :  \n",
            " - max token : 50 \n",
            " - temperature : 1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM setting"
      ],
      "metadata": {
        "id": "y3l2bNmu7wVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Comletion\n",
        "# prompt = PromptTemplate.from_template( '{time} + \\n\\n text{name} +  {action}')\n",
        "# user_input = 'Where shall we go today?'\n",
        "# new_prompt = prompt.format(\n",
        "#     time = \"old\",\n",
        "#     name = \"Genie\",\n",
        "#     action = user_input\n",
        "# )\n",
        "# res_llm = llm.invoke(new_prompt)\n",
        "\n",
        "# # Chat\n",
        "# template = \"You are my new friend. We met {place} for {activity}.\"\n",
        "# human_template = \"{text}\"\n",
        "# chat_prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", template), # 값을 tuple로 전달.\n",
        "#     (\"human\", human_template),\n",
        "# ])\n",
        "\n",
        "# prompt = chat_prompt.format_messages(place=\"in library\", activity=\"being a study friend\", text=\"Hey, Sweety!\")\n",
        "# res_chat = chat_model.invoke(prompt).content"
      ],
      "metadata": {
        "id": "2y1TMKNK_9yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(prompt)\n",
        "# print(res_chat)"
      ],
      "metadata": {
        "id": "tht5wSZM7D21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "qUS12SEeMmiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## loader\n",
        "# 특정 폴더 안의 모든 파일을 불러올 수 있도록 구성.\n",
        "# from langchain_community.document_loaders import DirectoryLoader > 이건 다 불러올 수 있는건가..?\n",
        "# from langchain_community.document_loaders import UnstructuredPDFLoader # 바꿀 것\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"gdrive/MyDrive/unsu.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "pages[0]"
      ],
      "metadata": {
        "id": "CX0ALDI8W3TX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a043669b-c7f0-4fa9-c7e7-8e35739ff860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='운수좋은날\\n현진건\\n새침하게흐린품이눈이올듯하더니눈은아니오고얼다가만비가추\\n적추적내리는날이었다.\\n이날이야말로동소문안에서인력거꾼노릇을하는김첨지에게는오래간만\\n에도닥친운수좋은날이었다문안에거기도문밖은아니지만들어간답 . ( )\\n시는앞집마마님을전찻길까지모셔다드린것을비롯으로행여나손님이\\n있을까하고정류장에서어정어정하며내리는사람하나하나에게거의비는\\n듯한눈결을보내고있다가마침내교원인듯한양복쟁이를동광학교(東光\\n까지태워다주기로되었다 ) . 學校\\n첫번에삼십전둘째번에오십전아침댓바람에그리흉치않은일이 , -\\n었다그야말로재수가옴붙어서근열흘동안돈구경도못한김첨지는십 .\\n전짜리백동화서푼또는다섯푼이찰깍하고손바닥에떨어질제거의 ,\\n눈물을흘릴만큼기뻤었다더구나이날이때에이팔십전이라는돈이그 .\\n에게얼마나유용한지몰랐다컬컬한목에모주한잔도적실수있거니와 .\\n그보다도앓는아내에게설렁탕한그릇도사다줄수있음이다.\\n그의아내가기침으로쿨룩거리기는벌써달포가넘었다조밥도굶기를 .\\n먹다시피하는형편이니물론약한첩써본일이없다구태여쓰려면못 .\\n쓸바도아니로되그는병이란놈에게약을주어보내면재미를붙여서자\\n꾸온다는자기의신조 에어디까지충실하였다따라서의사에게보 () . 信條\\n인적이없으니무슨병인지는알수없으되반듯이누워가지고일어나기\\n는새로모로도못눕는걸보면중증은중증인듯병이이대도록심해지 .\\n기는열흘전에조밥을먹고체한때문이다그때도김첨지가오래간만에돈 .\\n을얻어서좁쌀한되와십전짜리나무한단을사다주었더니김첨지의\\n말에의지하면그오라질년이천방지축으로냄비에대고끓였다마음은.\\n급하고불길은달지않아채익지도않은것을그오라질년이숟가락은고\\n만두고손으로움켜서두뺨에주먹덩이같은혹이불거지도록누가빼앗을\\n듯이처박질하더니만그날저녁부터가슴이땡긴다배가켕긴다고눈을흡 ,\\n뜨고지랄병을하였다그때김첨지는열화와같이성을내며 . ,\\n에이오라질년조랑복은할수가없어못먹어병먹어서병어쩌 “, , , , !\\n란말이야왜눈을바루뜨지못해 ! !”\\n하고앓는이의뺨을한번후려갈겼다흡뜬눈은조금바루어졌건만이슬 .', metadata={'source': 'gdrive/MyDrive/unsu.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Transform (chunking)\n",
        "# with open(\"../../state_of_the_union.txt\") as f: # 이건 로컬파일일 때...이야기인가. loader와 무슨 관계?\n",
        "#     state_of_the_union = f.read()\n",
        "\n",
        "# from langchain_experimental.text_splitter import SemanticChunker > 이건 openai의 실험적 기능이라는데... 한번 써보고 싶음.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=30,\n",
        "    # length_function=len, > tiktoken 사용시에는 왠지... 비활성해야 함.\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# texts = text_splitter.create_documents([state_of_the_union])\n",
        "texts = text_splitter.split_documents(pages)\n",
        "# print(texts[0])\n",
        "print(texts[0])\n",
        "print(texts[1])"
      ],
      "metadata": {
        "id": "ydyG5pytW4eR",
        "outputId": "b1966aca-1524-4e8a-f03d-138710f53c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='운수좋은날\\n현진건\\n새침하게흐린품이눈이올듯하더니눈은아니오고얼다가만비가추\\n적추적내리는날이었다.\\n이날이야말로동소문안에서인력거꾼노릇을하는김첨지에게는오래간만' metadata={'source': 'gdrive/MyDrive/unsu.pdf', 'page': 0}\n",
            "page_content='에도닥친운수좋은날이었다문안에거기도문밖은아니지만들어간답 . ( )\\n시는앞집마마님을전찻길까지모셔다드린것을비롯으로행여나손님이\\n있을까하고정류장에서어정어정하며내리는사람하나하나에게거의비는' metadata={'source': 'gdrive/MyDrive/unsu.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding & Store to vectorDB\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# chroma_client = chromadb.Client() > 이건 뭐에 필요한 것인지...\n",
        "embeddings_model = OpenAIEmbeddings(api_key=openai_api_key)\n",
        "\n",
        "db = Chroma.from_documents(texts, embeddings_model)\n",
        "\n",
        "user_input_embedding = input('검색할 키워드 입력 :')\n",
        "query = user_input_embedding\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "9msJUuwkrOZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a0c8c57-d25f-42ef-c112-f183e0052ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검색할 키워드 입력 :아내가 먹고 싶은 것은?\n",
            "그보다도앓는아내에게설렁탕한그릇도사다줄수있음이다.\n",
            "그의아내가기침으로쿨룩거리기는벌써달포가넘었다조밥도굶기를 .\n",
            "먹다시피하는형편이니물론약한첩써본일이없다구태여쓰려면못 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Store\n",
        "# Done with embedding"
      ],
      "metadata": {
        "id": "qqdCA6d3W4PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Retrieve from DB\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# user_input_multiquery = input('검색할 키워드 입력 :')\n",
        "# question = user_input_multiquery\n",
        "# llm = chat_model\n",
        "\n",
        "# retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "#     retriever=db.as_retriever(), llm=llm\n",
        "# )\n",
        "# print(retriever_from_llm)\n",
        "# print(len(res))\n",
        "# res = retriever_from_llm.get_relevant_documents(query = question)"
      ],
      "metadata": {
        "id": "oIMGRq9DW4Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Make answer with LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "user_input_multiquery = input('검색할 키워드 입력 :')\n",
        "question = user_input_multiquery\n",
        "llm = chat_model\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())\n",
        "result = qa_chain.invoke({'query':question})\n",
        "# print(result)\n",
        "print(result['result'])"
      ],
      "metadata": {
        "id": "6hxvlaeM0FT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d5d0a9-0f25-4d47-e7cb-b72771c5d3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검색할 키워드 입력 :아내가 먹고 싶은 것은?\n",
            "아내가 먹고 싶어하는 것은 설렁탕입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import Textarea, widgets\n",
        "from IPython.display import display\n",
        "\n",
        "output_to_display = res.content\n",
        "\n",
        "# print(~) > res = ~     >> res.contents 등 가공. output_to_display >>\n",
        "# 다시 위쪽의 코드에 반영.\n",
        "\n",
        "output_widget = Textarea(\n",
        "    value=output_to_display,\n",
        "    layout=widgets.Layout(width=\"100%\", height='300px')  # Can change Height\n",
        ")\n",
        "display(output_widget)"
      ],
      "metadata": {
        "id": "aByHNSHx_HBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For wordwrap for out cell."
      ],
      "metadata": {
        "id": "uqiEJjBe69uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet ipywidgets IPython"
      ],
      "metadata": {
        "id": "p967ExOH-046"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from ipywidgets import Textarea, widgets\n",
        "# # from IPython.display import display\n",
        "\n",
        "# output_to_display = res.content\n",
        "\n",
        "# # print(~) > res = ~     >> res.contents 등 가공. output_to_display >>\n",
        "# # 다시 위쪽의 코드에 반영.\n",
        "\n",
        "# output_widget = Textarea(\n",
        "#     value=output_to_display,\n",
        "#     layout=widgets.Layout(width=\"100%\", height='300px')  # Can change Height\n",
        "# )\n",
        "# display(output_widget)"
      ],
      "metadata": {
        "id": "tDnG-zxvaHLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function KeepClicking(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(KeepClicking,60000)"
      ],
      "metadata": {
        "id": "3YUr0T1Ae8Cw"
      }
    }
  ]
}