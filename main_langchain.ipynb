{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTvx/gfUmve3Sz+M/Lio59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/gpt_writer/blob/colab/main_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API_KEY 설정.\n",
        "- colab 환경 : 오른쪽 메뉴 > 열쇠모양(유저시크릿) > OPENAI_API_KEY로 설정 후 해당 코드실행."
      ],
      "metadata": {
        "id": "b16hMmRB8et1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "print(\"Got the key\" if openai_api_key else \"Something goes wrong\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dut4MoJcVTuf",
        "outputId": "4135b311-2cc7-4977-b401-a261c0d07ac7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got the key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 50\n",
        "TEMP = 1.5\n",
        "# _______________"
      ],
      "metadata": {
        "id": "2y1TMKNK_9yi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init Lanchain llm-model"
      ],
      "metadata": {
        "id": "X5YNSwFI9kCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain langchain_core langchain-openai"
      ],
      "metadata": {
        "id": "j9uCVp5M1ENA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export OPENAI_API_KEY=openai_api_key\n",
        "# set OPENAI_API_KEY=openai_api_key # on windows\n",
        "# %env OPENAI_API_KEY=openai_api_key # in colab"
      ],
      "metadata": {
        "id": "NZPig9muJrXE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(api_key=openai_api_key)\n",
        "chat_model = ChatOpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "BtIUojKEIWho"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt\n",
        "The LLM send / returns a **string**, while the ChatModel send / returns a **message**"
      ],
      "metadata": {
        "id": "y2j3i2L3Bhou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM prompt"
      ],
      "metadata": {
        "id": "O6vh42oszKOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.템플릿을 만들고 > 2.각 변수에 값을 할당한다. > 3.invoke로 보낸다.\n",
        "```python\n",
        "# from langchain.prompts import PromptTemplate # defrecated\n",
        "from langchain_core.proppts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template( 'text {var}' + '\\n\\n text{var} +  {input})\n",
        "new_prompt = prompt.format(  \n",
        "    time = \"old\",  \n",
        "    input = user_input  \n",
        ")\n",
        "llm.invoke(new_prompt)   \n",
        "```"
      ],
      "metadata": {
        "id": "JmzZMmmD0jvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"hi! {text}\")\n",
        "prompt_simple = prompt.format(text=\"Nice to see you.\")\n",
        "\n",
        "print(f'question : {prompt_simple}')\n",
        "print('***********************************************')\n",
        "print(f'LLM answer : {llm.invoke(prompt_simple)}')\n",
        "# print('***********************************************')\n",
        "# print(f'prompt_simple answer : {chat_model.invoke(prompt_simple).content}') # 사실 chat으로 호출해도 달라지는건 딱히.\n",
        "\n",
        "# 템플릿을 만들고\n",
        "prompt_vari01 = (\n",
        "    PromptTemplate.from_template('You are a {time} friend of mine')\n",
        "    + \" and live in {place}.\"\n",
        "    + \" We are a {relationship}\"\n",
        "    + \" What are we doing in {input}?\"\n",
        "    + \"\\n\\nAnswer me in just 1 sentence not any more.\"\n",
        ")\n",
        "\n",
        "prompt_vari02 = PromptTemplate.from_template(\n",
        "    'You are a {time} friend of mine'\n",
        "    + \" and live in {place}.\"\n",
        "    + \" We are a {relationship}\"\n",
        "    + \" What are we doing in {input}?\"\n",
        "    + \"\\n\\nAnswer me in just 1 sentence not any more.\"\n",
        "    )\n",
        "\n",
        "# 변수를 짝지어주고\n",
        "user_input = input('where shall we go? : ')\n",
        "\n",
        "prompt_vari01 = prompt_vari01.format(\n",
        "    time = \"old\",\n",
        "    place = \"New york\",\n",
        "    relationship = \"long-time no see status\",\n",
        "    input = user_input\n",
        ")\n",
        "prompt_vari02 = prompt_vari02.format(\n",
        "    time = \"old\",\n",
        "    place = \"New york\",\n",
        "    relationship = \"long-time no see status\",\n",
        "    input = user_input\n",
        ")\n",
        "\n",
        "# 보낸다.\n",
        "# print('***********************************************')\n",
        "# print(f'given : {prompt_vari01}')\n",
        "# print(f'prompt_vari01 answer : {chat_model.invoke(prompt_vari01).content}')\n",
        "# print('***********************************************')\n",
        "# print(f'given : {prompt_vari02}')\n",
        "# print(f'prompt_vari02 answer : {chat_model.invoke(prompt_vari02).content}')"
      ],
      "metadata": {
        "id": "lRcdm_G_yevV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat prompt"
      ],
      "metadata": {
        "id": "JYLgDlaYzQYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chat prompt도 구성방법은 llm(complettion)의 경우와 동일하다.   \n",
        "1.템플릿을 만들고 > 2.각 변수에 값을 할당한다. > 3.invoke로 보낸다.\n",
        "\n",
        "하지만 return 되서 돌아오는 값이 객체에 담겨오므로... **res.contets**로 꺼내써야 한다.\n",
        "```python\n",
        "# from langchain.prompts.chat import ChatPromptTemplate # defrecated\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"You are my new friend. We met {input_place} for {output_activity}.\"\n",
        "human_template = \"{text}\"\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template), # 값을 tuple로 전달.\n",
        "    (\"human\", human_template),\n",
        "])\n",
        "chat_prompt.format_messages(input_place=\"in library\", output_activity=\"being a study friend\", text=\"Hey, Sweety!\")\n",
        "chat_model.invoke(prompt).content\n",
        "```"
      ],
      "metadata": {
        "id": "s3h_3h1aRVA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 02\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = 'You are a {time} friend of mine. and live in {place} and now, We are a {relationship}. My friend would respond to me in one sentence, no more.'\n",
        "human_template = \"\\n\\nI want to go to {user_input}\"\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(  # v.[ from_template ] for llm.\n",
        "    [\n",
        "        ('system', system_template),\n",
        "        ('human', 'Where shall we go, today?'), # 이건 아마도 샘플일 것 같은데...\n",
        "        ('ai', 'As you go, my darling'),\n",
        "        ('human', human_template),\n",
        "    ]\n",
        ")\n",
        "# 아래와 같이 잘 말려 들어가 있는 걸 발견할 수 있다.\n",
        "# HumanMessage(content='Where shall we go, today?')\n",
        "\n",
        "user_input = input('where shall we go? : ')\n",
        "\n",
        "prompt = chat_template.format_messages( # v. [ prompt.format ] for llm\n",
        "    time = \"old\",\n",
        "    place = \"New york\",\n",
        "    relationship = \"good mood\",\n",
        "    user_input = user_input\n",
        ")\n",
        "\n",
        "print(f'me : {prompt}')\n",
        "print(f'Chat answer : {chat_model.invoke(prompt).content}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2t3po1B3Zjd",
        "outputId": "7687df6c-50f9-4582-f6ef-309716dc5ff0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where shall we go? : book store\n",
            "me : [SystemMessage(content='You are a old friend of mine. and live in New york and now, We are a good mood. My friend would respond to me in one sentence, no more.'), HumanMessage(content='Where shall we go, today?'), AIMessage(content='As you go, my darling'), HumanMessage(content='\\n\\nI want to go to book store')]\n",
            "Chat answer : Let's get lost in the pages of new stories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG\n",
        "1. 데이터를 올리고\n",
        "2. 잘라서\n",
        "3. 임베딩하고\n",
        "4. 크로마에 저장하고\n",
        "5. llm으로 질문과 추려낸 데이터를 보낸다."
      ],
      "metadata": {
        "id": "Cg8jQAmcVZqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chroma DB 연결.\n",
        "!pip install --quiet chromadb"
      ],
      "metadata": {
        "id": "jUitsnBlUq79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "chroma_client = chromadb.Client()"
      ],
      "metadata": {
        "id": "SHM6VgLMWX5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet TextLoader\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "EzBg69dpZRqZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loader\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"./index.md\")\n",
        "loader.load()"
      ],
      "metadata": {
        "id": "CX0ALDI8W3TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[\n",
        "    Document(\n",
        "        page_content='---\\nsidebar_position: 0\\n---\\n# Document loaders\\n\\nUse document loaders to load data from a source \\\n",
        "        as `Document`\\'s. A `Document` is a piece of text\\nand associated metadata. For example, there are document loaders \\\n",
        "        for loading a simple `.txt` file, for loading the text\\ncontents of any web page, or even for loading a transcript of\\\n",
        "         a YouTube video.\\n\\nEvery document loader exposes two methods:\\n1. \"Load\": load documents from the configured source\\\n",
        "         \\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\n\\n\\\n",
        "         They optionally implement:\\n\\n3. \"Lazy load\": load documents into memory lazily\\n',\n",
        "        metadata={'source': '../docs/docs/modules/data_connection/document_loaders/index.md'}\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "GCu-HQwcZxg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "Y6uejEAYZgnS",
        "outputId": "bf822843-75e3-473f-c31b-0197853297f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4996ee3d8d09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Transform (chunking)"
      ],
      "metadata": {
        "id": "ydyG5pytW4eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Embed"
      ],
      "metadata": {
        "id": "SnP7da8LW4Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Store"
      ],
      "metadata": {
        "id": "qqdCA6d3W4PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Retrieve"
      ],
      "metadata": {
        "id": "oIMGRq9DW4Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issue"
      ],
      "metadata": {
        "id": "2VE_kK2Z0q7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## format_instructions\n",
        "prompt의 Output 의 형태를 정해줄 수도 있다. `\\n\\n{format_instructions}`  \n",
        "아마도 몇개를 보여줘라... 할 때 어떻게 보여줄지를 알려주는 대목인 모양.\n",
        "그런데 이건 output parser 에서도 정해줄 수 있고 물론 prompt자체에서 해결할 수도 있다. 왜 여기서 굳이?"
      ],
      "metadata": {
        "id": "J6_jXyB3jWdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\" # \\n\\n 는 왜 필요한거지?\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_template(template)\n",
        "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions()) # format_instructions에 따라서 출력해라 ? 굳이 왜 이렇게 하는지 모르겠음.\n",
        "chain = chat_prompt | chat_model | output_parser\n",
        "chain.invoke({\"text\": \"human_name\"}) # 위의 teamplate > text 에 human_name을 넣어서 대답을 가져와라."
      ],
      "metadata": {
        "id": "LOmbXdfapVe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## schema\n",
        "● 일단 포기.\n",
        "schema와 template가 뭐가 다른 건지 모르겠음.  \n",
        "> 둘다 일종의 foramtting을 위한 것인데...\n",
        "\n",
        "\n",
        "문서에 따르자면, 순수하게 개발경험향상을 위해서 추가한 것이라고 한다. 하지만 다음과 같이 권고하고 있다.\n",
        "\n",
        "- 단순한 메시지 전달을 위해서는 `Message`를 쓰고,\n",
        "- 뭔가 변수를 통해 전달해야 할 것이 있는 경우에는 `MessageTemplate`을 활용하라고 한다."
      ],
      "metadata": {
        "id": "xDJEu0Qbjouf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "# llm.invoke(\"hi?\")\n",
        "\n",
        "# 단순한 프롬프트\n",
        "# message = \"Hi?\"\n",
        "# llm.invoke(message)\n",
        "\n",
        "# # 복잡한 프롬프트\n",
        "# template = 'You are a {time} friend of mine'\n",
        "# human_template = 'And I want you to {activity}'\n",
        "# ai_answered = 'Sure! why not!'\n",
        "\n",
        "# prompt = (\n",
        "#     SystemMessage(content = template) + HumanMessage(content = human_template) + AIMessage(content = ai_answered)\n",
        "# )\n",
        "\n",
        "# message01 = prompt.format_messages(\n",
        "#     time = \"old\",\n",
        "#     activity = \"help my love life\",\n",
        "# )\n",
        "\n",
        "# message02 = prompt.format_prompt(\n",
        "#     time = \"old\",\n",
        "#     activity = \"help my love life\",\n",
        "# )\n",
        "\n",
        "# prompt02 = SystemMessage(content=\"You are a nice pirate\")\n",
        "# prompt02_ = (\n",
        "#     prompt02 + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\"\n",
        "# )\n",
        "# message03 = prompt02_.format_messages(input=\"i said hi\")\n",
        "# res = new_prompt.format_messages(input=\"i said hi\")\n",
        "\n",
        "# print(f'format_messages_prompt : {message01}')\n",
        "# print(f'answer : {chat_model.invoke(message01)}')\n",
        "# print('********************************************')\n",
        "# print(f'format_prompt_prompt : {message02}')\n",
        "# print(f'answer : {chat_model.invoke(message02)}')\n",
        "# print('********************************************')\n",
        "# print(f'given : {message03}')\n",
        "# print(f'answer : {chat_model.invoke(message03)}')\n",
        "\n",
        "# print(f'res : {res}')"
      ],
      "metadata": {
        "id": "SW4czU513NMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(api_key=openai_api_key)\n",
        "chain = LLMChain(llm=model, prompt=prompt)\n",
        "chain.run(topic=\"sports\", language=\"korean\")"
      ],
      "metadata": {
        "id": "KwYe1h9C1wCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "VSWMU6QEWo2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding output parser"
      ],
      "metadata": {
        "id": "5OUS2oRrz5lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "output_parser.parse(\"hi, bye\")\n",
        "# >> ['hi', 'bye']"
      ],
      "metadata": {
        "id": "vLFfsyfkpFcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are new friend of mine\"),\n",
        "    (\"user\", \"{user_input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "RiO4m1S0332o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "SGquU3VA5ahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "vjzeRAn75ayX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_answered = chain.invoke({\"user_input\": \"Hi, there!\"})\n",
        "print(f'New friend from LLMs : {ai_answered}')"
      ],
      "metadata": {
        "id": "bxrEooe75qhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot prompt templates"
      ],
      "metadata": {
        "id": "NQUDxg7SEpRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Page link found | Content | As is link | to be link|\n",
        "------------------|-----------|---------|-----------|\n",
        "https://python.langchain.com/docs/modules/model_io/prompts | How to use few-shot examples with LLMs | https://python.langchain.com/docs/modules/model_io/few_shot_examples |https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples\n"
      ],
      "metadata": {
        "id": "O2dBQD-XwdvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a short {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"love\")"
      ],
      "metadata": {
        "id": "jUwARsnZHRbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "Kt5hmYn1E02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Hi, honey?\",\n",
        "        \"answer\": \"\"\"\n",
        "Yes,\n",
        "my love.\n",
        "When did you get here?\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"We just arrived. What should we eat?\",\n",
        "        \"answer\": \"\"\"\n",
        "Now I'm very hungry, so I think I can eat anything.\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Honey, What is your favorite animation these days?\",\n",
        "        \"answer\": \"\"\"\n",
        "You know I like \"Love Live\", right? A new season is on air these days.\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Really? Then do you want to go see it together?\",\n",
        "        \"answer\": \"\"\"\n",
        "No, it's okay. You're busy, so let's wait until the next movie comes out and watch it together.\n",
        "\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
        ")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))"
      ],
      "metadata": {
        "id": "GMOk9LqzE3oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For wordwrap for out cell."
      ],
      "metadata": {
        "id": "uqiEJjBe69uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # from ipywidgets import Textarea, widgets\n",
        "# # from IPython.display import display\n",
        "\n",
        "# output_to_display = res.content\n",
        "\n",
        "# # print(~) > res = ~     >> res.contents 등 가공. output_to_display >>\n",
        "# # 다시 위쪽의 코드에 반영.\n",
        "\n",
        "# output_widget = Textarea(\n",
        "#     value=output_to_display,\n",
        "#     layout=widgets.Layout(width=\"100%\", height='300px')  # Can change Height\n",
        "# )\n",
        "# display(output_widget)"
      ],
      "metadata": {
        "id": "tDnG-zxvaHLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function KeepClicking(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(KeepClicking,60000)"
      ],
      "metadata": {
        "id": "3YUr0T1Ae8Cw"
      }
    }
  ]
}