{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genie0320/gpt_writer/blob/colab/langchain_Bolier_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API_KEY 설정.\n",
        "- colab 환경 : 오른쪽 메뉴 > 열쇠모양(유저시크릿) > OPENAI_API_KEY로 설정 후 해당 코드실행."
      ],
      "metadata": {
        "id": "b16hMmRB8et1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "print(\"Got the key\" if openai_api_key else \"Something goes wrong\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dut4MoJcVTuf",
        "outputId": "38525bf5-5d01-4990-8802-4397d28c30c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got the key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ___ setting ___\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "MAX = 50\n",
        "TEMP = 1.5\n",
        "# _______________"
      ],
      "metadata": {
        "id": "2y1TMKNK_9yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet icecream\n",
        "import icecream as ic"
      ],
      "metadata": {
        "id": "FyK1fnnq1nhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init Lanchain llm-model"
      ],
      "metadata": {
        "id": "X5YNSwFI9kCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain langchain_core langchain-openai"
      ],
      "metadata": {
        "id": "j9uCVp5M1ENA",
        "outputId": "e07bb3fd-e767-4a92-d9be-579a65ada6f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/803.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/803.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.0/803.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export OPENAI_API_KEY=openai_api_key\n",
        "# set OPENAI_API_KEY=openai_api_key # on windows\n",
        "# %env OPENAI_API_KEY=openai_api_key # in colab"
      ],
      "metadata": {
        "id": "NZPig9muJrXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(api_key=openai_api_key)\n",
        "chat_model = ChatOpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "BtIUojKEIWho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt\n",
        "The LLM send / returns a **string**, while the ChatModel send / returns a **message**"
      ],
      "metadata": {
        "id": "y2j3i2L3Bhou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM prompt"
      ],
      "metadata": {
        "id": "O6vh42oszKOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.템플릿을 만들고 > 2.각 변수에 값을 할당한다. > 3.invoke로 보낸다.\n",
        "```python\n",
        "# from langchain.prompts import PromptTemplate # defrecated\n",
        "from langchain_core.proppts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template( 'text {var}' + '\\n\\n text{var} +  {input})\n",
        "new_prompt = prompt.format(  \n",
        "    time = \"old\",  \n",
        "    input = user_input  \n",
        ")\n",
        "llm.invoke(new_prompt)   \n",
        "```"
      ],
      "metadata": {
        "id": "JmzZMmmD0jvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"hi! {text}\")\n",
        "prompt_simple = prompt.format(text=\"Nice to see you.\")\n",
        "\n",
        "print(f'question : {prompt_simple}')\n",
        "print('***********************************************')\n",
        "print(f'LLM answer : {llm.invoke(prompt_simple)}')\n",
        "# print('***********************************************')\n",
        "# print(f'prompt_simple answer : {chat_model.invoke(prompt_simple).content}') # 사실 chat으로 호출해도 달라지는건 딱히.\n",
        "\n",
        "# 템플릿을 만들고\n",
        "prompt_vari01 = (\n",
        "    PromptTemplate.from_template('You are a {time} friend of mine')\n",
        "    + \" and live in {place}.\"\n",
        "    + \" We are a {relationship}\"\n",
        "    + \" What are we doing in {input}?\"\n",
        "    + \"\\n\\nAnswer me in just 1 sentence not any more.\"\n",
        ")\n",
        "\n",
        "prompt_vari02 = PromptTemplate.from_template(\n",
        "    'You are a {time} friend of mine'\n",
        "    + \" and live in {place}.\"\n",
        "    + \" We are a {relationship}\"\n",
        "    + \" What are we doing in {input}?\"\n",
        "    + \"\\n\\nAnswer me in just 1 sentence not any more.\"\n",
        "    )\n",
        "\n",
        "# 변수를 짝지어주고\n",
        "user_input = input('where shall we go? : ')\n",
        "\n",
        "prompt_vari01 = prompt_vari01.format(\n",
        "    time = \"old\",\n",
        "    place = \"New york\",\n",
        "    relationship = \"long-time no see status\",\n",
        "    input = user_input\n",
        ")\n",
        "prompt_vari02 = prompt_vari02.format(\n",
        "    time = \"old\",\n",
        "    place = \"New york\",\n",
        "    relationship = \"long-time no see status\",\n",
        "    input = user_input\n",
        ")\n",
        "\n",
        "# 보낸다.\n",
        "# print('***********************************************')\n",
        "# print(f'given : {prompt_vari01}')\n",
        "# print(f'prompt_vari01 answer : {chat_model.invoke(prompt_vari01).content}')\n",
        "# print('***********************************************')\n",
        "# print(f'given : {prompt_vari02}')\n",
        "# print(f'prompt_vari02 answer : {chat_model.invoke(prompt_vari02).content}')"
      ],
      "metadata": {
        "id": "lRcdm_G_yevV",
        "outputId": "b342f12d-ade2-429e-9868-b664d4c126d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question : hi! Nice to see you.\n",
            "***********************************************\n",
            "LLM answer : \n",
            "\n",
            "Hello! It's nice to see you too. How have you been?\n",
            "where shall we go? : hotel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat prompt"
      ],
      "metadata": {
        "id": "JYLgDlaYzQYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chat prompt도 구성방법은 llm(complettion)의 경우와 동일하다.   \n",
        "1.템플릿을 만들고 > 2.각 변수에 값을 할당한다. > 3.invoke로 보낸다.\n",
        "\n",
        "하지만 return 되서 돌아오는 값이 객체에 담겨오므로... **res.contets**로 꺼내써야 한다.\n",
        "```python\n",
        "# from langchain.prompts.chat import ChatPromptTemplate # defrecated\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"You are my new friend. We met {input_place} for {output_activity}.\"\n",
        "human_template = \"{text}\"\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template), # 값을 tuple로 전달.\n",
        "    (\"human\", human_template),\n",
        "])\n",
        "chat_prompt.format_messages(input_place=\"in library\", output_activity=\"being a study friend\", text=\"Hey, Sweety!\")\n",
        "chat_model.invoke(prompt).content\n",
        "```"
      ],
      "metadata": {
        "id": "s3h_3h1aRVA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 02\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = 'You are a {time} friend of mine. and live in {place} and now, We are a {relationship}. My friend would respond to me in one sentence, no more.'\n",
        "human_template = \"\\n\\nI want to go to {user_input}\"\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(  # v.[ from_template ] for llm.\n",
        "    [\n",
        "        ('system', system_template),\n",
        "        ('human', 'Where shall we go, today?'), # 이건 아마도 샘플일 것 같은데...\n",
        "        ('ai', 'As you go, my darling'),\n",
        "        ('human', human_template),\n",
        "    ]\n",
        ")\n",
        "# 아래와 같이 잘 말려 들어가 있는 걸 발견할 수 있다.\n",
        "# HumanMessage(content='Where shall we go, today?')\n",
        "\n",
        "user_input = input('where shall we go? : ')\n",
        "\n",
        "prompt = chat_template.format_messages( # v. [ prompt.format ] for llm\n",
        "    time = \"old\",\n",
        "    place = \"New york\",\n",
        "    relationship = \"good mood\",\n",
        "    user_input = user_input\n",
        ")\n",
        "\n",
        "print(f'me : {prompt}')\n",
        "print(f'Chat answer : {chat_model.invoke(prompt).content}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2t3po1B3Zjd",
        "outputId": "b147da5c-67d6-4773-9a19-78d71ddf7fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where shall we go? : hotel\n",
            "me : [SystemMessage(content='You are a old friend of mine. and live in New york and now, We are a good mood. My friend would respond to me in one sentence, no more.'), HumanMessage(content='Where shall we go, today?'), AIMessage(content='As you go, my darling'), HumanMessage(content='\\n\\nI want to go to hotel')]\n",
            "Chat answer : Sure, let's book a hotel and have a great time!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG\n",
        "1. 데이터를 올리고\n",
        "2. 잘라서\n",
        "3. 임베딩하고\n",
        "4. 크로마에 저장하고\n",
        "5. llm으로 질문과 추려낸 데이터를 보낸다."
      ],
      "metadata": {
        "id": "Cg8jQAmcVZqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set reaource data Storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ObB2hAS7gD4l",
        "outputId": "2ff47b21-a62d-4546-ca93-6cc35e6a6633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet TextLoader\n",
        "# !pip install --quiet pypdf faiss-gpu"
      ],
      "metadata": {
        "id": "EzBg69dpZRqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loader\n",
        "# 특정 폴더 안의 모든 파일을 불러올 수 있도록 구성.\n",
        "# from langchain_community.document_loaders import DirectoryLoader > 이건 다 불러올 수 있는건가..?\n",
        "# from langchain_community.document_loaders import UnstructuredPDFLoader # 바꿀 것\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"gdrive/MyDrive/unsu.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "pages[0]"
      ],
      "metadata": {
        "id": "CX0ALDI8W3TX",
        "outputId": "9fafff85-612c-494f-ec1e-c75f2e0d09aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='운수좋은날\\n현진건\\n새침하게흐린품이눈이올듯하더니눈은아니오고얼다가만비가추\\n적추적내리는날이었다.\\n이날이야말로동소문안에서인력거꾼노릇을하는김첨지에게는오래간만\\n에도닥친운수좋은날이었다문안에거기도문밖은아니지만들어간답 . ( )\\n시는앞집마마님을전찻길까지모셔다드린것을비롯으로행여나손님이\\n있을까하고정류장에서어정어정하며내리는사람하나하나에게거의비는\\n듯한눈결을보내고있다가마침내교원인듯한양복쟁이를동광학교(東光\\n까지태워다주기로되었다 ) . 學校\\n첫번에삼십전둘째번에오십전아침댓바람에그리흉치않은일이 , -\\n었다그야말로재수가옴붙어서근열흘동안돈구경도못한김첨지는십 .\\n전짜리백동화서푼또는다섯푼이찰깍하고손바닥에떨어질제거의 ,\\n눈물을흘릴만큼기뻤었다더구나이날이때에이팔십전이라는돈이그 .\\n에게얼마나유용한지몰랐다컬컬한목에모주한잔도적실수있거니와 .\\n그보다도앓는아내에게설렁탕한그릇도사다줄수있음이다.\\n그의아내가기침으로쿨룩거리기는벌써달포가넘었다조밥도굶기를 .\\n먹다시피하는형편이니물론약한첩써본일이없다구태여쓰려면못 .\\n쓸바도아니로되그는병이란놈에게약을주어보내면재미를붙여서자\\n꾸온다는자기의신조 에어디까지충실하였다따라서의사에게보 () . 信條\\n인적이없으니무슨병인지는알수없으되반듯이누워가지고일어나기\\n는새로모로도못눕는걸보면중증은중증인듯병이이대도록심해지 .\\n기는열흘전에조밥을먹고체한때문이다그때도김첨지가오래간만에돈 .\\n을얻어서좁쌀한되와십전짜리나무한단을사다주었더니김첨지의\\n말에의지하면그오라질년이천방지축으로냄비에대고끓였다마음은.\\n급하고불길은달지않아채익지도않은것을그오라질년이숟가락은고\\n만두고손으로움켜서두뺨에주먹덩이같은혹이불거지도록누가빼앗을\\n듯이처박질하더니만그날저녁부터가슴이땡긴다배가켕긴다고눈을흡 ,\\n뜨고지랄병을하였다그때김첨지는열화와같이성을내며 . ,\\n에이오라질년조랑복은할수가없어못먹어병먹어서병어쩌 “, , , , !\\n란말이야왜눈을바루뜨지못해 ! !”\\n하고앓는이의뺨을한번후려갈겼다흡뜬눈은조금바루어졌건만이슬 .', metadata={'source': 'gdrive/MyDrive/unsu.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Transform (chunking)\n",
        "# with open(\"../../state_of_the_union.txt\") as f: # 이건 로컬파일일 때...이야기인가. loader와 무슨 관계?\n",
        "#     state_of_the_union = f.read()\n",
        "\n",
        "# from langchain_experimental.text_splitter import SemanticChunker > 이건 openai의 실험적 기능이라는데... 한번 써보고 싶음.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=30,\n",
        "    # length_function=len, > tiktoken 사용시에는 왠지... 비활성해야 함.\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# texts = text_splitter.create_documents([state_of_the_union])\n",
        "texts = text_splitter.split_documents(pages)\n",
        "# print(texts[0])\n",
        "print(texts[0])\n",
        "print(texts[1])"
      ],
      "metadata": {
        "id": "ydyG5pytW4eR",
        "outputId": "367cfee3-07c1-4952-aeb3-febd46643c65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='운수좋은날\\n현진건\\n새침하게흐린품이눈이올듯하더니눈은아니오고얼다가만비가추\\n적추적내리는날이었다.\\n이날이야말로동소문안에서인력거꾼노릇을하는김첨지에게는오래간만' metadata={'source': 'gdrive/MyDrive/unsu.pdf', 'page': 0}\n",
            "page_content='에도닥친운수좋은날이었다문안에거기도문밖은아니지만들어간답 . ( )\\n시는앞집마마님을전찻길까지모셔다드린것을비롯으로행여나손님이\\n있을까하고정류장에서어정어정하며내리는사람하나하나에게거의비는' metadata={'source': 'gdrive/MyDrive/unsu.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chroma DB 설치.\n",
        "# !pip install --quiet chromadb"
      ],
      "metadata": {
        "id": "jUitsnBlUq79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding & Store to vectorDB\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# chroma_client = chromadb.Client() > 이건 뭐에 필요한 것인지...\n",
        "embeddings_model = OpenAIEmbeddings(api_key=openai_api_key)\n",
        "\n",
        "db = Chroma.from_documents(texts, embeddings_model)\n",
        "\n",
        "user_input_embedding = input('검색할 키워드 입력 :')\n",
        "query = user_input_embedding\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "9msJUuwkrOZO",
        "outputId": "ba8ce74b-547e-4735-d3b0-15f2c7e4aa4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검색할 키워드 입력 :설렁탕\n",
            "설렁탕을사다놓았는데왜먹지를못하니왜먹지를못하니괴상 “ , ……\n",
            "하게도오늘은운수가좋더니만 ! , .”……\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Store\n",
        "# Done with embedding"
      ],
      "metadata": {
        "id": "qqdCA6d3W4PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Retrieve from DB\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# user_input_multiquery = input('검색할 키워드 입력 :')\n",
        "# question = user_input_multiquery\n",
        "# llm = chat_model\n",
        "\n",
        "# retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "#     retriever=db.as_retriever(), llm=llm\n",
        "# )\n",
        "# print(retriever_from_llm)\n",
        "# print(len(res))\n",
        "# res = retriever_from_llm.get_relevant_documents(query = question)"
      ],
      "metadata": {
        "id": "oIMGRq9DW4Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Make answer with LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "user_input_multiquery = input('검색할 키워드 입력 :')\n",
        "question = user_input_multiquery\n",
        "llm = chat_model\n",
        "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())\n",
        "result = qa_chain({'query':question})\n",
        "# print(result)\n",
        "print(result['result'])"
      ],
      "metadata": {
        "id": "6hxvlaeM0FT8",
        "outputId": "4d9d9b0f-caa3-4e07-cdaa-13764725e36b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검색할 키워드 입력 :아내가 먹고 싶어하는 것은?\n",
            "아내가 먹고 싶어하는 것은 설렁탕입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issue"
      ],
      "metadata": {
        "id": "2VE_kK2Z0q7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## format_instructions\n",
        "prompt의 Output 의 형태를 정해줄 수도 있다. `\\n\\n{format_instructions}`  \n",
        "아마도 몇개를 보여줘라... 할 때 어떻게 보여줄지를 알려주는 대목인 모양.\n",
        "그런데 이건 output parser 에서도 정해줄 수 있고 물론 prompt자체에서 해결할 수도 있다. 왜 여기서 굳이?"
      ],
      "metadata": {
        "id": "J6_jXyB3jWdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Generate a list of 5 {text}.\\n\\n{format_instructions}\" # \\n\\n 는 왜 필요한거지?\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_template(template)\n",
        "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions()) # format_instructions에 따라서 출력해라 ? 굳이 왜 이렇게 하는지 모르겠음.\n",
        "chain = chat_prompt | chat_model | output_parser\n",
        "chain.invoke({\"text\": \"human_name\"}) # 위의 teamplate > text 에 human_name을 넣어서 대답을 가져와라."
      ],
      "metadata": {
        "id": "LOmbXdfapVe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## schema\n",
        "● 일단 포기.\n",
        "schema와 template가 뭐가 다른 건지 모르겠음.  \n",
        "> 둘다 일종의 foramtting을 위한 것인데...\n",
        "\n",
        "\n",
        "문서에 따르자면, 순수하게 개발경험향상을 위해서 추가한 것이라고 한다. 하지만 다음과 같이 권고하고 있다.\n",
        "\n",
        "- 단순한 메시지 전달을 위해서는 `Message`를 쓰고,\n",
        "- 뭔가 변수를 통해 전달해야 할 것이 있는 경우에는 `MessageTemplate`을 활용하라고 한다."
      ],
      "metadata": {
        "id": "xDJEu0Qbjouf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "# llm.invoke(\"hi?\")\n",
        "\n",
        "# 단순한 프롬프트\n",
        "# message = \"Hi?\"\n",
        "# llm.invoke(message)\n",
        "\n",
        "# # 복잡한 프롬프트\n",
        "# template = 'You are a {time} friend of mine'\n",
        "# human_template = 'And I want you to {activity}'\n",
        "# ai_answered = 'Sure! why not!'\n",
        "\n",
        "# prompt = (\n",
        "#     SystemMessage(content = template) + HumanMessage(content = human_template) + AIMessage(content = ai_answered)\n",
        "# )\n",
        "\n",
        "# message01 = prompt.format_messages(\n",
        "#     time = \"old\",\n",
        "#     activity = \"help my love life\",\n",
        "# )\n",
        "\n",
        "# message02 = prompt.format_prompt(\n",
        "#     time = \"old\",\n",
        "#     activity = \"help my love life\",\n",
        "# )\n",
        "\n",
        "# prompt02 = SystemMessage(content=\"You are a nice pirate\")\n",
        "# prompt02_ = (\n",
        "#     prompt02 + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\"\n",
        "# )\n",
        "# message03 = prompt02_.format_messages(input=\"i said hi\")\n",
        "# res = new_prompt.format_messages(input=\"i said hi\")\n",
        "\n",
        "# print(f'format_messages_prompt : {message01}')\n",
        "# print(f'answer : {chat_model.invoke(message01)}')\n",
        "# print('********************************************')\n",
        "# print(f'format_prompt_prompt : {message02}')\n",
        "# print(f'answer : {chat_model.invoke(message02)}')\n",
        "# print('********************************************')\n",
        "# print(f'given : {message03}')\n",
        "# print(f'answer : {chat_model.invoke(message03)}')\n",
        "\n",
        "# print(f'res : {res}')"
      ],
      "metadata": {
        "id": "SW4czU513NMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(api_key=openai_api_key)\n",
        "chain = LLMChain(llm=model, prompt=prompt)\n",
        "chain.run(topic=\"sports\", language=\"korean\")"
      ],
      "metadata": {
        "id": "KwYe1h9C1wCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "VSWMU6QEWo2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding output parser"
      ],
      "metadata": {
        "id": "5OUS2oRrz5lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "output_parser.parse(\"hi, bye\")\n",
        "# >> ['hi', 'bye']"
      ],
      "metadata": {
        "id": "vLFfsyfkpFcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are new friend of mine\"),\n",
        "    (\"user\", \"{user_input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "RiO4m1S0332o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "SGquU3VA5ahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "vjzeRAn75ayX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_answered = chain.invoke({\"user_input\": \"Hi, there!\"})\n",
        "print(f'New friend from LLMs : {ai_answered}')"
      ],
      "metadata": {
        "id": "bxrEooe75qhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot prompt templates"
      ],
      "metadata": {
        "id": "NQUDxg7SEpRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Page link found | Content | As is link | to be link|\n",
        "------------------|-----------|---------|-----------|\n",
        "https://python.langchain.com/docs/modules/model_io/prompts | How to use few-shot examples with LLMs | https://python.langchain.com/docs/modules/model_io/few_shot_examples |https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples\n"
      ],
      "metadata": {
        "id": "O2dBQD-XwdvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a short {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"love\")"
      ],
      "metadata": {
        "id": "jUwARsnZHRbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "Kt5hmYn1E02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Hi, honey?\",\n",
        "        \"answer\": \"\"\"\n",
        "Yes,\n",
        "my love.\n",
        "When did you get here?\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"We just arrived. What should we eat?\",\n",
        "        \"answer\": \"\"\"\n",
        "Now I'm very hungry, so I think I can eat anything.\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Honey, What is your favorite animation these days?\",\n",
        "        \"answer\": \"\"\"\n",
        "You know I like \"Love Live\", right? A new season is on air these days.\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Really? Then do you want to go see it together?\",\n",
        "        \"answer\": \"\"\"\n",
        "No, it's okay. You're busy, so let's wait until the next movie comes out and watch it together.\n",
        "\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
        ")\n",
        "\n",
        "print(example_prompt.format(**examples[0]))"
      ],
      "metadata": {
        "id": "GMOk9LqzE3oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For wordwrap for out cell."
      ],
      "metadata": {
        "id": "uqiEJjBe69uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # from ipywidgets import Textarea, widgets\n",
        "# # from IPython.display import display\n",
        "\n",
        "# output_to_display = res.content\n",
        "\n",
        "# # print(~) > res = ~     >> res.contents 등 가공. output_to_display >>\n",
        "# # 다시 위쪽의 코드에 반영.\n",
        "\n",
        "# output_widget = Textarea(\n",
        "#     value=output_to_display,\n",
        "#     layout=widgets.Layout(width=\"100%\", height='300px')  # Can change Height\n",
        "# )\n",
        "# display(output_widget)"
      ],
      "metadata": {
        "id": "tDnG-zxvaHLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function KeepClicking(){\n",
        "console.log(\"Clicking\");\n",
        "document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(KeepClicking,60000)"
      ],
      "metadata": {
        "id": "3YUr0T1Ae8Cw"
      }
    }
  ]
}